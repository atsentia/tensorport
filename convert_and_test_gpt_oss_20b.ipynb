{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-OSS-20B: Convert to JAX and Test Inference\n",
    "\n",
    "This notebook converts the downloaded GPT-OSS-20B model from safetensors format to JAX-compatible NumPy arrays and runs a test inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import Dict, Any\n",
    "import psutil\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install -q safetensors transformers jax jaxlib psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = \"/mnt/example-runs-vol/gpt-oss-20b\"\n",
    "OUTPUT_PATH = \"/mnt/example-runs-vol/gpt-oss-20b-jax\"\n",
    "TENSORPORT_PATH = \"/mnt/example-runs-vol/tensorport\"\n",
    "\n",
    "print(f\"Model path: {MODEL_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build TensorPort (Rust Converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and build TensorPort if not already present\n",
    "if not os.path.exists(TENSORPORT_PATH):\n",
    "    print(\"Cloning TensorPort repository...\")\n",
    "    !git clone https://github.com/your-username/tensorport.git {TENSORPORT_PATH}\n",
    "    \n",
    "# Install Rust if not already installed\n",
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "!source $HOME/.cargo/env\n",
    "\n",
    "# Build TensorPort\n",
    "print(\"Building TensorPort...\")\n",
    "os.chdir(TENSORPORT_PATH)\n",
    "!$HOME/.cargo/bin/cargo build --release\n",
    "\n",
    "tensorport_binary = f\"{TENSORPORT_PATH}/target/release/tensorport\"\n",
    "print(f\"TensorPort binary: {tensorport_binary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert Model to JAX Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024 / 1024 / 1024  # GB\n",
    "\n",
    "print(f\"Memory before conversion: {get_memory_usage():.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TensorPort conversion\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "conversion_cmd = [\n",
    "    tensorport_binary,\n",
    "    \"convert\",\n",
    "    \"--input\", MODEL_PATH,\n",
    "    \"--output\", OUTPUT_PATH,\n",
    "    \"--format\", \"numpy-direct\",\n",
    "    \"--precision\", \"float16\",\n",
    "    \"--verbose\"\n",
    "]\n",
    "\n",
    "print(\"Starting conversion...\")\n",
    "print(f\"Command: {' '.join(conversion_cmd)}\")\n",
    "\n",
    "start_time = time.time()\n",
    "result = subprocess.run(conversion_cmd, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"✅ Conversion successful!\")\n",
    "    print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"❌ Conversion failed!\")\n",
    "    print(f\"Error: {result.stderr}\")\n",
    "    \n",
    "print(f\"Memory after conversion: {get_memory_usage():.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List converted files\n",
    "output_files = list(Path(OUTPUT_PATH).glob(\"*.npy\"))\n",
    "print(f\"\\nConverted {len(output_files)} tensor files:\")\n",
    "for f in sorted(output_files)[:10]:  # Show first 10\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  {f.name}: {size_mb:.2f} MB\")\n",
    "if len(output_files) > 10:\n",
    "    print(f\"  ... and {len(output_files) - 10} more files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model config\n",
    "config_path = Path(MODEL_PATH) / \"config.json\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Architecture: {config.get('architectures', ['Unknown'])[0]}\")\n",
    "print(f\"  Hidden size: {config.get('hidden_size', 'N/A')}\")\n",
    "print(f\"  Num layers: {config.get('num_hidden_layers', 'N/A')}\")\n",
    "print(f\"  Num heads: {config.get('num_attention_heads', 'N/A')}\")\n",
    "print(f\"  Vocab size: {config.get('vocab_size', 'N/A')}\")\n",
    "print(f\"  Max position embeddings: {config.get('max_position_embeddings', 'N/A')}\")\n",
    "\n",
    "# Check if it's MoE\n",
    "if 'num_experts' in config:\n",
    "    print(f\"\\nMixture of Experts:\")\n",
    "    print(f\"  Num experts: {config['num_experts']}\")\n",
    "    print(f\"  Experts per token: {config.get('num_experts_per_tok', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simple JAX Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGPTModel:\n",
    "    \"\"\"Simple GPT model for testing weight loading and basic inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, weights_path: str, config: Dict[str, Any]):\n",
    "        self.weights_path = Path(weights_path)\n",
    "        self.config = config\n",
    "        self.weights = {}\n",
    "        \n",
    "    def load_embedding_weights(self):\n",
    "        \"\"\"Load just the embedding weights for a simple test.\"\"\"\n",
    "        print(\"Loading embedding weights...\")\n",
    "        \n",
    "        # Try to find embedding weight file\n",
    "        embed_files = list(self.weights_path.glob(\"*embed*.npy\"))\n",
    "        if not embed_files:\n",
    "            embed_files = list(self.weights_path.glob(\"*token*.npy\"))\n",
    "        \n",
    "        if embed_files:\n",
    "            embed_path = embed_files[0]\n",
    "            print(f\"Loading {embed_path.name}...\")\n",
    "            embed_weight = np.load(embed_path)\n",
    "            self.weights['embed'] = jnp.array(embed_weight)\n",
    "            print(f\"  Shape: {self.weights['embed'].shape}\")\n",
    "            print(f\"  Dtype: {self.weights['embed'].dtype}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Could not find embedding weights\")\n",
    "            return False\n",
    "    \n",
    "    def simple_embed(self, token_ids):\n",
    "        \"\"\"Simple embedding lookup.\"\"\"\n",
    "        if 'embed' not in self.weights:\n",
    "            raise ValueError(\"Embedding weights not loaded\")\n",
    "        \n",
    "        # Simple embedding lookup\n",
    "        embeddings = self.weights['embed'][token_ids]\n",
    "        return embeddings\n",
    "\n",
    "# Create model instance\n",
    "model = SimpleGPTModel(OUTPUT_PATH, config)\n",
    "\n",
    "# Load embedding weights\n",
    "if model.load_embedding_weights():\n",
    "    print(\"\\n✅ Successfully loaded embedding weights!\")\n",
    "else:\n",
    "    print(\"\\n❌ Failed to load embedding weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embedding lookup\n",
    "if 'embed' in model.weights:\n",
    "    print(\"Testing embedding lookup...\")\n",
    "    \n",
    "    # Create some test token IDs\n",
    "    test_tokens = jnp.array([1, 100, 1000, 5000], dtype=jnp.int32)\n",
    "    print(f\"Test tokens: {test_tokens}\")\n",
    "    \n",
    "    # Run embedding lookup\n",
    "    start_time = time.time()\n",
    "    embeddings = model.simple_embed(test_tokens)\n",
    "    embed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nEmbedding results:\")\n",
    "    print(f\"  Output shape: {embeddings.shape}\")\n",
    "    print(f\"  Output dtype: {embeddings.dtype}\")\n",
    "    print(f\"  Time taken: {embed_time*1000:.2f} ms\")\n",
    "    print(f\"  Mean value: {jnp.mean(embeddings):.6f}\")\n",
    "    print(f\"  Std dev: {jnp.std(embeddings):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_embedding_lookup(model, num_tokens=1024, num_iterations=100):\n",
    "    \"\"\"Benchmark embedding lookup performance.\"\"\"\n",
    "    if 'embed' not in model.weights:\n",
    "        print(\"Embedding weights not loaded\")\n",
    "        return\n",
    "    \n",
    "    vocab_size = model.weights['embed'].shape[0]\n",
    "    \n",
    "    # Generate random token IDs\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    token_ids = jax.random.randint(key, (num_tokens,), 0, vocab_size)\n",
    "    \n",
    "    # Warmup\n",
    "    print(f\"Warming up with {10} iterations...\")\n",
    "    for _ in range(10):\n",
    "        _ = model.simple_embed(token_ids)\n",
    "    \n",
    "    # Benchmark\n",
    "    print(f\"\\nBenchmarking {num_iterations} iterations with {num_tokens} tokens...\")\n",
    "    times = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        start = time.time()\n",
    "        embeddings = model.simple_embed(token_ids)\n",
    "        embeddings.block_until_ready()  # Wait for JAX computation\n",
    "        times.append(time.time() - start)\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"  Iteration {i+1}/{num_iterations}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    times = np.array(times)\n",
    "    \n",
    "    print(f\"\\n📊 Benchmark Results:\")\n",
    "    print(f\"  Mean time: {np.mean(times)*1000:.2f} ms\")\n",
    "    print(f\"  Median time: {np.median(times)*1000:.2f} ms\")\n",
    "    print(f\"  Min time: {np.min(times)*1000:.2f} ms\")\n",
    "    print(f\"  Max time: {np.max(times)*1000:.2f} ms\")\n",
    "    print(f\"  Std dev: {np.std(times)*1000:.2f} ms\")\n",
    "    print(f\"  Throughput: {num_tokens / np.mean(times):.0f} tokens/second\")\n",
    "\n",
    "# Run benchmark\n",
    "if 'embed' in model.weights:\n",
    "    benchmark_embedding_lookup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load and Test More Layers (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layer_weights(weights_path: Path, layer_idx: int = 0):\n",
    "    \"\"\"Load weights for a specific transformer layer.\"\"\"\n",
    "    layer_weights = {}\n",
    "    \n",
    "    # Common patterns for layer weight files\n",
    "    patterns = [\n",
    "        f\"*layer.{layer_idx}*.npy\",\n",
    "        f\"*layers.{layer_idx}*.npy\",\n",
    "        f\"*h.{layer_idx}*.npy\",\n",
    "        f\"*block.{layer_idx}*.npy\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nSearching for layer {layer_idx} weights...\")\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        files = list(weights_path.glob(pattern))\n",
    "        if files:\n",
    "            print(f\"Found {len(files)} files matching pattern: {pattern}\")\n",
    "            for f in files[:5]:  # Show first 5\n",
    "                weight = np.load(f)\n",
    "                key = f.stem.replace('.', '_')\n",
    "                layer_weights[key] = jnp.array(weight)\n",
    "                print(f\"  Loaded {key}: shape={weight.shape}, dtype={weight.dtype}\")\n",
    "            break\n",
    "    \n",
    "    return layer_weights\n",
    "\n",
    "# Try to load first layer weights\n",
    "layer_0_weights = load_layer_weights(Path(OUTPUT_PATH), layer_idx=0)\n",
    "\n",
    "if layer_0_weights:\n",
    "    print(f\"\\n✅ Successfully loaded {len(layer_0_weights)} weights for layer 0\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Could not find layer 0 weights (this is okay for initial test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Memory and Storage Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_size(path):\n",
    "    \"\"\"Calculate total size of directory.\"\"\"\n",
    "    total_size = 0\n",
    "    for f in Path(path).rglob('*'):\n",
    "        if f.is_file():\n",
    "            total_size += f.stat().st_size\n",
    "    return total_size / 1024 / 1024 / 1024  # GB\n",
    "\n",
    "# Calculate sizes\n",
    "original_size = get_directory_size(MODEL_PATH)\n",
    "converted_size = get_directory_size(OUTPUT_PATH)\n",
    "\n",
    "print(\"\\n📦 Storage Summary:\")\n",
    "print(f\"  Original model size: {original_size:.2f} GB\")\n",
    "print(f\"  Converted model size: {converted_size:.2f} GB\")\n",
    "print(f\"  Compression ratio: {(1 - converted_size/original_size)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n💾 Memory Usage:\")\n",
    "print(f\"  Current memory: {get_memory_usage():.2f} GB\")\n",
    "print(f\"  Available memory: {psutil.virtual_memory().available / 1024 / 1024 / 1024:.2f} GB\")\n",
    "print(f\"  Total system memory: {psutil.virtual_memory().total / 1024 / 1024 / 1024:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🎯 Conversion and Test Complete!\\n\")\n",
    "print(\"Next steps:\")\n",
    "print(\"1. ✅ Model successfully converted to JAX format\")\n",
    "print(\"2. ✅ Basic embedding lookup working\")\n",
    "print(\"3. 📋 TODO: Implement full model architecture in JAX\")\n",
    "print(\"4. 📋 TODO: Add MXFP4 quantization support\")\n",
    "print(\"5. 📋 TODO: Implement text generation\")\n",
    "print(\"6. 📋 TODO: Compare with PyTorch baseline\")\n",
    "\n",
    "print(\"\\n📊 Performance expectations:\")\n",
    "print(\"  - CPU: ~1-2 seconds per token (current)\")\n",
    "print(\"  - T4 GPU: ~100-200 tokens/second (expected)\")\n",
    "print(\"  - A10G GPU: ~200-400 tokens/second (expected)\")\n",
    "print(\"  - With MXFP4: Additional 2-4x speedup\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}