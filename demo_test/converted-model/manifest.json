{
  "config": {
    "head_dim": 64,
    "hidden_act": "silu",
    "hidden_size": 768,
    "intermediate_size": 768,
    "layer_types": [
      "attention",
      "attention",
      "attention",
      "attention"
    ],
    "max_position_embeddings": 4096,
    "num_attention_heads": 12,
    "num_experts_per_tok": 2,
    "num_hidden_layers": 4,
    "num_key_value_heads": 4,
    "num_local_experts": 8,
    "quantization_method": "none",
    "rms_norm_eps": 1e-6,
    "rope_theta": 150000.0,
    "sliding_window": 128,
    "tie_word_embeddings": true,
    "use_kv_cache": true,
    "vocab_size": 32000
  },
  "format": "numpy_direct",
  "tensors": [
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_3_self_attn_v_proj_weight.npy",
      "name": "model.layers.3.self_attn.v_proj.weight",
      "shape": [
        256,
        768
      ],
      "size_mb": 0.375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_mlp_gate_proj_weight.npy",
      "name": "model.layers.0.mlp.gate_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_mlp_gate_proj_weight.npy",
      "name": "model.layers.2.mlp.gate_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_input_layernorm_weight.npy",
      "name": "model.layers.0.input_layernorm.weight",
      "shape": [
        768
      ],
      "size_mb": 0.00146484375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_self_attn_o_proj_weight.npy",
      "name": "model.layers.1.self_attn.o_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_post_attention_layernorm_weight.npy",
      "name": "model.layers.2.post_attention_layernorm.weight",
      "shape": [
        768
      ],
      "size_mb": 0.00146484375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_3_mlp_down_proj_weight.npy",
      "name": "model.layers.3.mlp.down_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_3_self_attn_q_proj_weight.npy",
      "name": "model.layers.3.self_attn.q_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_mlp_down_proj_weight.npy",
      "name": "model.layers.0.mlp.down_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_mlp_down_proj_weight.npy",
      "name": "model.layers.1.mlp.down_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_mlp_up_proj_weight.npy",
      "name": "model.layers.2.mlp.up_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_self_attn_o_proj_weight.npy",
      "name": "model.layers.0.self_attn.o_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_norm_weight.npy",
      "name": "model.norm.weight",
      "shape": [
        768
      ],
      "size_mb": 0.00146484375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_3_self_attn_o_proj_weight.npy",
      "name": "model.layers.3.self_attn.o_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_self_attn_k_proj_weight.npy",
      "name": "model.layers.1.self_attn.k_proj.weight",
      "shape": [
        256,
        768
      ],
      "size_mb": 0.375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_3_self_attn_k_proj_weight.npy",
      "name": "model.layers.3.self_attn.k_proj.weight",
      "shape": [
        256,
        768
      ],
      "size_mb": 0.375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_input_layernorm_weight.npy",
      "name": "model.layers.2.input_layernorm.weight",
      "shape": [
        768
      ],
      "size_mb": 0.00146484375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_input_layernorm_weight.npy",
      "name": "model.layers.1.input_layernorm.weight",
      "shape": [
        768
      ],
      "size_mb": 0.00146484375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_3_mlp_up_proj_weight.npy",
      "name": "model.layers.3.mlp.up_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_self_attn_v_proj_weight.npy",
      "name": "model.layers.0.self_attn.v_proj.weight",
      "shape": [
        256,
        768
      ],
      "size_mb": 0.375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_post_attention_layernorm_weight.npy",
      "name": "model.layers.0.post_attention_layernorm.weight",
      "shape": [
        768
      ],
      "size_mb": 0.00146484375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_self_attn_k_proj_weight.npy",
      "name": "model.layers.0.self_attn.k_proj.weight",
      "shape": [
        256,
        768
      ],
      "size_mb": 0.375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_mlp_gate_proj_weight.npy",
      "name": "model.layers.1.mlp.gate_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_self_attn_o_proj_weight.npy",
      "name": "model.layers.2.self_attn.o_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_embed_tokens_weight.npy",
      "name": "model.embed_tokens.weight",
      "shape": [
        32000,
        768
      ],
      "size_mb": 46.875
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_mlp_down_proj_weight.npy",
      "name": "model.layers.2.mlp.down_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_self_attn_q_proj_weight.npy",
      "name": "model.layers.2.self_attn.q_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_post_attention_layernorm_weight.npy",
      "name": "model.layers.1.post_attention_layernorm.weight",
      "shape": [
        768
      ],
      "size_mb": 0.00146484375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_self_attn_v_proj_weight.npy",
      "name": "model.layers.1.self_attn.v_proj.weight",
      "shape": [
        256,
        768
      ],
      "size_mb": 0.375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_3_input_layernorm_weight.npy",
      "name": "model.layers.3.input_layernorm.weight",
      "shape": [
        768
      ],
      "size_mb": 0.00146484375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_self_attn_k_proj_weight.npy",
      "name": "model.layers.2.self_attn.k_proj.weight",
      "shape": [
        256,
        768
      ],
      "size_mb": 0.375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_mlp_up_proj_weight.npy",
      "name": "model.layers.0.mlp.up_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_mlp_up_proj_weight.npy",
      "name": "model.layers.1.mlp.up_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_3_mlp_gate_proj_weight.npy",
      "name": "model.layers.3.mlp.gate_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_self_attn_v_proj_weight.npy",
      "name": "model.layers.2.self_attn.v_proj.weight",
      "shape": [
        256,
        768
      ],
      "size_mb": 0.375
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_self_attn_q_proj_weight.npy",
      "name": "model.layers.0.self_attn.q_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_self_attn_q_proj_weight.npy",
      "name": "model.layers.1.self_attn.q_proj.weight",
      "shape": [
        768,
        768
      ],
      "size_mb": 1.125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_3_post_attention_layernorm_weight.npy",
      "name": "model.layers.3.post_attention_layernorm.weight",
      "shape": [
        768
      ],
      "size_mb": 0.00146484375
    }
  ],
  "total_parameters": 37952256,
  "total_shards": 1
}