{
  "timestamp": "2025-08-06 21:27:53 UTC",
  "success": false,
  "errors": [
    "Conversion failed: Error: Json(Error(\"missing field `total_size_gb`\", line: 403, column: 1))\n",
    "Critical error: Conversion failed: Conversion failed: Error: Json(Error(\"missing field `total_size_gb`\", line: 403, column: 1))\n"
  ],
  "model_config": {
    "vocab_size": 32000,
    "hidden_size": 768,
    "num_hidden_layers": 4,
    "num_attention_heads": 12,
    "num_key_value_heads": 4,
    "head_dim": 64,
    "intermediate_size": 768,
    "num_local_experts": 8,
    "num_experts_per_tok": 2,
    "hidden_act": "silu",
    "max_position_embeddings": 4096,
    "rope_theta": 150000.0,
    "sliding_window": 128,
    "rms_norm_eps": 1e-06,
    "quantization_method": "none",
    "use_kv_cache": true,
    "tie_word_embeddings": true,
    "layer_types": [
      "attention",
      "attention",
      "attention",
      "attention"
    ]
  },
  "generation_stats": {
    "total_parameters": "37952256",
    "total_tensors": 38,
    "file_size_mb": 72.39209175109863,
    "quantization": "fp16"
  },
  "generation_time": 1.0466992855072021,
  "conversion_time": 0.21531295776367188,
  "conversion_success": false,
  "conversion_message": "Conversion failed: Error: Json(Error(\"missing field `total_size_gb`\", line: 403, column: 1))\n",
  "validation_results": {
    "tensors_loaded": 0
  },
  "inference_results": {
    "inference_successful": false,
    "sample_outputs": []
  },
  "total_duration": 1.2621548175811768
}