{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download GPT-OSS-20B from Hugging Face\n",
    "\n",
    "This notebook downloads the GPT-OSS-20B model from Hugging Face to a Modal volume.\n",
    "\n",
    "## Prerequisites\n",
    "- Modal.com account with GPU access\n",
    "- Attached Modal volume for model storage\n",
    "- ~13GB of storage space for the quantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q huggingface_hub tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Download Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "MODEL_ID = \"openai/gpt-oss-20b\"\n",
    "LOCAL_DIR = \"/mnt/models/gpt-oss-20b\"  # Adjust this path to your Modal volume mount point\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Model will be downloaded to: {LOCAL_DIR}\")\n",
    "print(f\"Model ID: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Model Files\n",
    "\n",
    "This will download all the safetensors files and configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the entire model repository\n",
    "print(\"Starting download...\")\n",
    "print(\"This may take several minutes depending on your connection speed.\")\n",
    "print(\"Expected download size: ~13GB\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # Download all model files\n",
    "    downloaded_path = snapshot_download(\n",
    "        repo_id=MODEL_ID,\n",
    "        local_dir=LOCAL_DIR,\n",
    "        local_dir_use_symlinks=False,\n",
    "        resume_download=True,\n",
    "        ignore_patterns=[\"*.md\", \"*.txt\", \".git*\"],  # Skip documentation files\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ Model downloaded successfully to: {downloaded_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error downloading model: {e}\")\n",
    "    print(\"\\nTrying alternative download method...\")\n",
    "    \n",
    "    # Alternative: Download specific files\n",
    "    try:\n",
    "        # First, get the index file to know which shards to download\n",
    "        index_file = hf_hub_download(\n",
    "            repo_id=MODEL_ID,\n",
    "            filename=\"model.safetensors.index.json\",\n",
    "            local_dir=LOCAL_DIR,\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        \n",
    "        # Download config files\n",
    "        for config_file in [\"config.json\", \"tokenizer_config.json\", \"tokenizer.json\"]:\n",
    "            try:\n",
    "                hf_hub_download(\n",
    "                    repo_id=MODEL_ID,\n",
    "                    filename=config_file,\n",
    "                    local_dir=LOCAL_DIR,\n",
    "                    local_dir_use_symlinks=False\n",
    "                )\n",
    "                print(f\"✓ Downloaded {config_file}\")\n",
    "            except:\n",
    "                print(f\"⚠ Could not download {config_file} (might not exist)\")\n",
    "        \n",
    "        # Read the index to get list of shard files\n",
    "        with open(os.path.join(LOCAL_DIR, \"model.safetensors.index.json\"), 'r') as f:\n",
    "            index_data = json.load(f)\n",
    "        \n",
    "        # Get unique shard files\n",
    "        shard_files = set(index_data.get('weight_map', {}).values())\n",
    "        \n",
    "        print(f\"\\nDownloading {len(shard_files)} shard files...\")\n",
    "        for shard_file in sorted(shard_files):\n",
    "            print(f\"Downloading {shard_file}...\")\n",
    "            hf_hub_download(\n",
    "                repo_id=MODEL_ID,\n",
    "                filename=shard_file,\n",
    "                local_dir=LOCAL_DIR,\n",
    "                local_dir_use_symlinks=False,\n",
    "                resume_download=True\n",
    "            )\n",
    "            print(f\"✓ Downloaded {shard_file}\")\n",
    "        \n",
    "        print(f\"\\n✅ Model downloaded successfully to: {LOCAL_DIR}\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Alternative download also failed: {e2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    \"\"\"Calculate total size of directory.\"\"\"\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            if os.path.exists(filepath):\n",
    "                total += os.path.getsize(filepath)\n",
    "    return total\n",
    "\n",
    "def format_size(bytes):\n",
    "    \"\"\"Format bytes to human readable string.\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if bytes < 1024.0:\n",
    "            return f\"{bytes:.2f} {unit}\"\n",
    "        bytes /= 1024.0\n",
    "    return f\"{bytes:.2f} PB\"\n",
    "\n",
    "# List downloaded files\n",
    "print(\"Downloaded files:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "model_path = Path(LOCAL_DIR)\n",
    "if model_path.exists():\n",
    "    files = sorted(model_path.glob(\"*\"))\n",
    "    \n",
    "    total_size = 0\n",
    "    for file in files:\n",
    "        if file.is_file():\n",
    "            size = file.stat().st_size\n",
    "            total_size += size\n",
    "            print(f\"{file.name:50} {format_size(size):>10}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total files: {len(files)}\")\n",
    "    print(f\"Total size: {format_size(total_size)}\")\n",
    "    \n",
    "    # Check for critical files\n",
    "    critical_files = [\n",
    "        \"config.json\",\n",
    "        \"model.safetensors.index.json\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nCritical files check:\")\n",
    "    for critical_file in critical_files:\n",
    "        file_path = model_path / critical_file\n",
    "        if file_path.exists():\n",
    "            print(f\"✅ {critical_file} - Found\")\n",
    "        else:\n",
    "            print(f\"❌ {critical_file} - Missing\")\n",
    "    \n",
    "    # Count safetensors shards\n",
    "    shard_files = list(model_path.glob(\"*.safetensors\"))\n",
    "    if shard_files:\n",
    "        print(f\"\\n✅ Found {len(shard_files)} safetensors shard files\")\n",
    "    else:\n",
    "        print(\"\\n⚠ No safetensors files found\")\n",
    "        \n",
    "else:\n",
    "    print(f\"❌ Directory {LOCAL_DIR} does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model Configuration\n",
    "\n",
    "Let's inspect the model configuration to understand its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config_path = os.path.join(LOCAL_DIR, \"config.json\")\n",
    "\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    print(\"Model Configuration:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Key configuration details\n",
    "    key_configs = [\n",
    "        \"architectures\",\n",
    "        \"hidden_size\",\n",
    "        \"intermediate_size\",\n",
    "        \"num_attention_heads\",\n",
    "        \"num_hidden_layers\",\n",
    "        \"num_key_value_heads\",\n",
    "        \"vocab_size\",\n",
    "        \"max_position_embeddings\",\n",
    "        \"torch_dtype\",\n",
    "        \"quantization_config\"\n",
    "    ]\n",
    "    \n",
    "    for key in key_configs:\n",
    "        if key in config:\n",
    "            value = config[key]\n",
    "            if isinstance(value, dict):\n",
    "                print(f\"{key}:\")\n",
    "                for k, v in value.items():\n",
    "                    print(f\"  {k}: {v}\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Calculate parameter count\n",
    "    if \"num_hidden_layers\" in config and \"hidden_size\" in config:\n",
    "        layers = config[\"num_hidden_layers\"]\n",
    "        hidden = config[\"hidden_size\"]\n",
    "        vocab = config.get(\"vocab_size\", 0)\n",
    "        \n",
    "        print(\"\\nModel Statistics:\")\n",
    "        print(f\"- Layers: {layers}\")\n",
    "        print(f\"- Hidden size: {hidden}\")\n",
    "        print(f\"- Vocabulary: {vocab:,} tokens\")\n",
    "        \n",
    "        # Check for MoE configuration\n",
    "        if \"num_local_experts\" in config:\n",
    "            print(f\"- Experts: {config['num_local_experts']} (Mixture of Experts)\")\n",
    "            print(f\"- Active experts: {config.get('num_experts_per_tok', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"❌ Config file not found at {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check Quantization Format\n",
    "\n",
    "Let's verify if the model uses MXFP4 quantization as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the safetensors index for weight information\n",
    "index_path = os.path.join(LOCAL_DIR, \"model.safetensors.index.json\")\n",
    "\n",
    "if os.path.exists(index_path):\n",
    "    with open(index_path, 'r') as f:\n",
    "        index_data = json.load(f)\n",
    "    \n",
    "    print(\"Model Weight Information:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get metadata\n",
    "    metadata = index_data.get('metadata', {})\n",
    "    if metadata:\n",
    "        print(\"Metadata:\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Analyze weight map\n",
    "    weight_map = index_data.get('weight_map', {})\n",
    "    \n",
    "    # Count different types of weights\n",
    "    weight_types = {}\n",
    "    for weight_name in weight_map.keys():\n",
    "        # Check for quantized weights (blocks and scales pattern)\n",
    "        if '.blocks' in weight_name:\n",
    "            weight_types['quantized_blocks'] = weight_types.get('quantized_blocks', 0) + 1\n",
    "        elif '.scales' in weight_name:\n",
    "            weight_types['quantized_scales'] = weight_types.get('quantized_scales', 0) + 1\n",
    "        elif '.weight' in weight_name:\n",
    "            weight_types['regular_weights'] = weight_types.get('regular_weights', 0) + 1\n",
    "        elif '.bias' in weight_name:\n",
    "            weight_types['biases'] = weight_types.get('biases', 0) + 1\n",
    "        else:\n",
    "            weight_types['other'] = weight_types.get('other', 0) + 1\n",
    "    \n",
    "    print(\"\\nWeight Statistics:\")\n",
    "    print(f\"Total weights: {len(weight_map)}\")\n",
    "    for wtype, count in sorted(weight_types.items()):\n",
    "        print(f\"  {wtype}: {count}\")\n",
    "    \n",
    "    # Check if model appears to be quantized\n",
    "    if weight_types.get('quantized_blocks', 0) > 0:\n",
    "        print(\"\\n✅ Model appears to use MXFP4 quantization\")\n",
    "        print(f\"   Found {weight_types.get('quantized_blocks', 0)} quantized blocks\")\n",
    "        print(f\"   Found {weight_types.get('quantized_scales', 0)} scale tensors\")\n",
    "    else:\n",
    "        print(\"\\n⚠ Model may not be quantized or uses different format\")\n",
    "    \n",
    "    # Show example weight names\n",
    "    print(\"\\nExample weight names (first 10):\")\n",
    "    for i, name in enumerate(list(weight_map.keys())[:10]):\n",
    "        print(f\"  {name}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"❌ Index file not found at {index_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "Now that the model is downloaded, you can:\n",
    "\n",
    "1. **Convert to JAX format**: Use TensorPort to convert the safetensors to JAX-compatible NumPy arrays\n",
    "2. **Run inference**: Load the model with JAX and run inference\n",
    "3. **Benchmark performance**: Test inference speed on different GPU types\n",
    "4. **Fine-tune**: Use the model for downstream tasks\n",
    "\n",
    "### Quick Conversion Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example command to convert using TensorPort (if installed)\n",
    "print(\"To convert the model to JAX format using TensorPort:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"tensorport convert \\\\\")\n",
    "print(f\"    --input {LOCAL_DIR} \\\\\")\n",
    "print(f\"    --output {LOCAL_DIR}-jax \\\\\")\n",
    "print(f\"    --format numpy-direct \\\\\")\n",
    "print(f\"    --precision float16\")\n",
    "print()\n",
    "print(\"This will create JAX-loadable NumPy arrays from the safetensors files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has downloaded the GPT-OSS-20B model from Hugging Face. The model uses MXFP4 quantization for efficient storage and inference.\n",
    "\n",
    "Key points:\n",
    "- Model size: ~13GB (quantized)\n",
    "- Architecture: Mixture of Experts with 21B total parameters, 3.6B active\n",
    "- Quantization: MXFP4 (4-bit with shared exponents)\n",
    "- Ready for conversion to JAX format using TensorPort"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}