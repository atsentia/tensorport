#!/usr/bin/env python3
"""
Test JAX inference with tokenization for meaningful output
Uses the model's tokenizer to create real inputs and decode outputs.
"""

import json
import numpy as np
import jax
import jax.numpy as jnp
from pathlib import Path
import time
from typing import Dict, Tuple, List
import warnings

def load_tensor(base_path: Path, tensor_name: str) -> jnp.ndarray:
    """Load a single tensor from the sharded numpy files."""
    file_name = tensor_name.replace('.', '_') + '.npy'
    
    for shard_dir in sorted(base_path.glob('shard_*')):
        tensor_path = shard_dir / file_name
        if tensor_path.exists():
            np_array = np.load(tensor_path)
            return jnp.array(np_array)
    
    return None

def load_tokenizer_config(model_path: Path) -> Dict:
    """Load tokenizer configuration from the original model."""
    tokenizer_path = Path('gpt-oss-20b/tokenizer.json')
    if tokenizer_path.exists():
        with open(tokenizer_path) as f:
            return json.load(f)
    return None

def simple_tokenize(text: str, vocab_size: int = 201088) -> List[int]:
    """
    Simple tokenization - convert text to token IDs.
    This is a simplified version that maps characters/words to IDs.
    """
    # For testing, we'll use a simple approach
    # In production, you'd use the actual tokenizer
    
    # Common tokens for GPT models
    token_map = {
        "Hello": 15496,
        "hello": 31373,
        "world": 1917,
        "World": 10846,
        "!": 999,
        ".": 13,
        ",": 11,
        " ": 220,  # space
        "The": 464,
        "the": 262,
        "is": 318,
        "a": 257,
        "I": 314,
        "am": 716,
        "AI": 9552,
        "model": 2746,
        "language": 3303,
        "How": 2437,
        "are": 389,
        "you": 345,
        "?": 30,
        "today": 1909,
        "weather": 6193,
        "nice": 3621,
    }
    
    # Tokenize by splitting and mapping
    tokens = []
    words = text.replace("?", " ?").replace("!", " !").replace(".", " .").replace(",", " ,").split()
    
    for word in words:
        if word in token_map:
            tokens.append(token_map[word])
        else:
            # Use a hash-based fallback for unknown words
            tokens.append(abs(hash(word)) % min(50000, vocab_size))
    
    return tokens

def decode_tokens(token_ids: List[int]) -> str:
    """
    Simple decoding - convert token IDs back to text.
    This is a reverse of our simple tokenization.
    """
    # Reverse token map
    reverse_map = {
        15496: "Hello",
        31373: "hello",
        1917: "world",
        10846: "World",
        999: "!",
        13: ".",
        11: ",",
        220: " ",
        464: "The",
        262: "the",
        318: "is",
        257: "a",
        314: "I",
        716: "am",
        9552: "AI",
        2746: "model",
        3303: "language",
        2437: "How",
        389: "are",
        345: "you",
        30: "?",
        1909: "today",
        6193: "weather",
        3621: "nice",
    }
    
    words = []
    for token_id in token_ids:
        if token_id in reverse_map:
            words.append(reverse_map[token_id])
        else:
            # For unknown tokens, use a placeholder
            words.append(f"<tok_{token_id}>")
    
    return " ".join(words)

def load_model_for_generation(model_path: Path) -> Tuple[Dict, Dict]:
    """Load model parameters needed for text generation."""
    print("Loading model for generation...")
    
    with open(model_path / 'manifest.json') as f:
        manifest = json.load(f)
    
    config = manifest['config']
    
    params = {}
    
    # Load essential parameters
    print("  Loading embeddings and output layer...")
    params['embed_tokens'] = load_tensor(model_path, 'model.embed_tokens.weight')
    params['lm_head'] = load_tensor(model_path, 'lm_head.weight')
    params['norm'] = load_tensor(model_path, 'model.norm.weight')
    
    # Load first few layers for testing
    params['layers'] = []
    for i in range(min(3, config['num_hidden_layers'])):  # Load first 3 layers
        print(f"  Loading layer {i}...")
        layer = {}
        layer['input_layernorm'] = load_tensor(model_path, f'model.layers.{i}.input_layernorm.weight')
        layer['post_attention_layernorm'] = load_tensor(model_path, f'model.layers.{i}.post_attention_layernorm.weight')
        
        # Attention weights
        layer['q_proj'] = load_tensor(model_path, f'model.layers.{i}.self_attn.q_proj.weight')
        layer['k_proj'] = load_tensor(model_path, f'model.layers.{i}.self_attn.k_proj.weight')
        layer['v_proj'] = load_tensor(model_path, f'model.layers.{i}.self_attn.v_proj.weight')
        layer['o_proj'] = load_tensor(model_path, f'model.layers.{i}.self_attn.o_proj.weight')
        
        params['layers'].append(layer)
    
    return params, config

def forward_pass_with_logits(params: Dict, config: Dict, input_ids: jnp.ndarray) -> jnp.ndarray:
    """Run forward pass and return logits for next token prediction."""
    batch_size, seq_len = input_ids.shape
    hidden_size = config['hidden_size']
    
    # Embedding lookup
    hidden_states = params['embed_tokens'][input_ids]
    
    # Pass through layers (simplified - only using first few layers)
    for layer_idx, layer in enumerate(params['layers']):
        # Skip if layer params are missing
        if layer['input_layernorm'] is None:
            continue
            
        # Simplified layer processing
        # Layer norm
        normed = hidden_states * layer['input_layernorm']
        
        # Self-attention (very simplified)
        if layer['q_proj'] is not None:
            q = jnp.matmul(normed, layer['q_proj'].T)  # (batch, seq, 4096)
            k = jnp.matmul(normed, layer['k_proj'].T)   # (batch, seq, 512)
            v = jnp.matmul(normed, layer['v_proj'].T)   # (batch, seq, 512)
            
            # For simplified attention, just use first hidden_size dims of q
            q_reduced = q[:, :, :512]  # Match k/v dimensions
            
            # Compute attention scores
            scores = jnp.matmul(q_reduced, k.transpose(0, 2, 1)) / jnp.sqrt(512)
            attn_weights = jax.nn.softmax(scores, axis=-1)
            attn_output = jnp.matmul(attn_weights, v)  # (batch, seq, 512)
            
            # Expand back to hidden size
            attn_output_expanded = jnp.pad(attn_output, ((0, 0), (0, 0), (0, hidden_size - 512)))
            
            # Output projection (if available)
            if layer['o_proj'] is not None and attn_output_expanded.shape[-1] == 4096:
                attn_output_expanded = jnp.matmul(attn_output_expanded, layer['o_proj'].T)
            
            # Residual connection
            hidden_states = hidden_states + attn_output_expanded[:, :, :hidden_size]
    
    # Final layer norm
    if params['norm'] is not None:
        hidden_states = hidden_states * params['norm']
    
    # Get logits from language model head
    if params['lm_head'] is not None:
        logits = jnp.matmul(hidden_states, params['lm_head'].T)
    else:
        # Fallback: use embedding matrix as output (common in some models)
        logits = jnp.matmul(hidden_states, params['embed_tokens'].T)
    
    return logits

def generate_text(params: Dict, config: Dict, prompt: str, max_new_tokens: int = 20) -> str:
    """Generate text given a prompt."""
    print(f"\nüéØ Generating text from prompt: '{prompt}'")
    
    # Tokenize input
    input_tokens = simple_tokenize(prompt)
    print(f"   Input tokens: {input_tokens}")
    
    # Convert to JAX array
    input_ids = jnp.array([input_tokens])
    generated_tokens = input_tokens.copy()
    
    # Generate tokens one by one
    for i in range(max_new_tokens):
        # Get logits for next token
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            logits = forward_pass_with_logits(params, config, input_ids)
        
        # Get next token (greedy decoding for simplicity)
        next_token_logits = logits[0, -1, :]  # Last position
        
        # Apply temperature and sample
        temperature = 0.8
        next_token_logits = next_token_logits / temperature
        probs = jax.nn.softmax(next_token_logits)
        
        # Sample from top-k
        top_k = 50
        top_k_probs, top_k_indices = jax.lax.top_k(probs, k=min(top_k, probs.shape[0]))
        top_k_probs = top_k_probs / jnp.sum(top_k_probs)
        
        # Sample
        next_token = top_k_indices[jnp.argmax(top_k_probs)]
        
        # Add to sequence
        generated_tokens.append(int(next_token))
        input_ids = jnp.array([generated_tokens])
        
        # Stop if we hit end token or get stuck
        if next_token == 2 or next_token == 0:  # Common end tokens
            break
    
    # Decode
    generated_text = decode_tokens(generated_tokens)
    return generated_text, generated_tokens

def main():
    print("="*60)
    print("JAX Inference with Tokenization Test")
    print("="*60)
    
    model_path = Path('jax-numpy-model')
    if not model_path.exists():
        print(f"‚ùå Model directory not found: {model_path}")
        return
    
    # Load model
    try:
        params, config = load_model_for_generation(model_path)
        print(f"\n‚úÖ Model loaded: {len(params['layers'])} layers")
    except Exception as e:
        print(f"‚ùå Failed to load model: {e}")
        return
    
    # Test prompts
    test_prompts = [
        "Hello world",
        "The weather is",
        "How are you",
        "I am a language",
    ]
    
    print("\n" + "="*60)
    print("GENERATION EXAMPLES")
    print("="*60)
    
    for prompt in test_prompts:
        try:
            generated_text, tokens = generate_text(params, config, prompt, max_new_tokens=10)
            
            print(f"\nüìù Prompt: '{prompt}'")
            print(f"   Tokens: {tokens[:len(simple_tokenize(prompt))]}")
            print(f"   ‚Üí Generated: '{generated_text}'")
            print(f"   ‚Üí New tokens: {tokens[len(simple_tokenize(prompt)):]}")
            
        except Exception as e:
            print(f"\n‚ùå Generation failed for '{prompt}': {e}")
    
    # Show model statistics
    print("\n" + "="*60)
    print("MODEL STATISTICS")
    print("="*60)
    
    if params['embed_tokens'] is not None:
        vocab_size = params['embed_tokens'].shape[0]
        embed_dim = params['embed_tokens'].shape[1]
        print(f"Vocabulary size: {vocab_size:,}")
        print(f"Embedding dimension: {embed_dim}")
    
    if params['lm_head'] is not None:
        output_size = params['lm_head'].shape[0]
        print(f"Output vocabulary: {output_size:,}")
    
    print("\n" + "="*60)
    print("‚ú® Test Complete!")
    print("="*60)

if __name__ == "__main__":
    main()