Metadata-Version: 2.4
Name: jax-mxfp4
Version: 0.1.0
Summary: High-performance 4-bit quantization library for JAX
Author-email: Atsentia Team <engineering@atsentia.com>
Maintainer-email: Atsentia Team <engineering@atsentia.com>
License: MIT
Project-URL: Homepage, https://github.com/atsentia/jax-mxfp4
Project-URL: Documentation, https://jax-mxfp4.readthedocs.io
Project-URL: Repository, https://github.com/atsentia/jax-mxfp4
Project-URL: Issues, https://github.com/atsentia/jax-mxfp4/issues
Keywords: jax,quantization,4-bit,mxfp4,deep-learning,machine-learning,neural-networks,compression,llm,transformers
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: jax>=0.4.20
Requires-Dist: jaxlib>=0.4.20
Requires-Dist: numpy>=1.21.0
Requires-Dist: flax>=0.7.0
Requires-Dist: optax>=0.1.7
Requires-Dist: chex>=0.1.8
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0; extra == "dev"
Requires-Dist: black>=23.0; extra == "dev"
Requires-Dist: isort>=5.12; extra == "dev"
Requires-Dist: mypy>=1.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Provides-Extra: triton
Requires-Dist: triton>=2.0; extra == "triton"
Requires-Dist: jax-triton>=0.1.4; extra == "triton"
Provides-Extra: tpu
Requires-Dist: jax[tpu]>=0.4.20; extra == "tpu"
Provides-Extra: cuda
Requires-Dist: jax[cuda12]>=0.4.20; extra == "cuda"
Provides-Extra: benchmarks
Requires-Dist: pandas>=1.5; extra == "benchmarks"
Requires-Dist: matplotlib>=3.5; extra == "benchmarks"
Requires-Dist: seaborn>=0.12; extra == "benchmarks"

# JAX-MXFP4: 4-bit Quantization for JAX

A high-performance 4-bit quantization library for JAX, optimized for large language models like GPT-OSS-20B.

## Features

- 🚀 **Native JAX/XLA Integration** - JIT-compilable kernels for maximum performance
- 📦 **MXFP4 Format** - 4-bit weights with shared exponents per block
- 🎯 **8x Compression** - Reduce model size from 86GB (FP32) to ~11GB
- ⚡ **Custom Kernels** - Optimized Pallas/Triton kernels for different GPUs
- 🔧 **Easy Integration** - Drop-in replacement for standard JAX layers
- 📊 **TPU Support** - Works on TPU, GPU, and CPU

## Installation

```bash
pip install jax-mxfp4
```

Or install from source:
```bash
git clone https://github.com/atsentia/jax-mxfp4.git
cd jax-mxfp4
pip install -e .
```

## Quick Start

### Basic Quantization

```python
import jax
import jax.numpy as jnp
from jax_mxfp4 import quantize_to_mxfp4, dequantize_mxfp4

# Quantize weights
weight = jax.random.normal(jax.random.PRNGKey(0), (4096, 4096))
packed, scales = quantize_to_mxfp4(weight, block_size=32)

# Dequantize when needed
weight_reconstructed = dequantize_mxfp4(packed, scales, weight.shape)

# Compression ratio
print(f"Compression: {weight.nbytes / (packed.nbytes + scales.nbytes):.1f}x")
```

### Model Conversion

```python
from jax_mxfp4 import convert_model_to_mxfp4, MXFP4Model

# Convert existing model
original_model = load_your_model()
mxfp4_model = convert_model_to_mxfp4(original_model)

# Or build MXFP4 model from scratch
model = MXFP4Model(
    hidden_size=4096,
    num_layers=32,
    num_heads=32,
    vocab_size=50257,
    block_size=32,  # MXFP4 block size
)

# Forward pass (JIT-compiled)
output = model(input_ids)
```

### Custom Layers

```python
from jax_mxfp4 import MXFP4Linear, MXFP4Attention

# Replace standard layers with MXFP4 versions
linear = MXFP4Linear(
    in_features=4096,
    out_features=4096,
    block_size=32,
)

# Initialize with random key
params = linear.init(jax.random.PRNGKey(0), jnp.ones((1, 4096)))

# Forward pass
output = linear.apply(params, input_tensor)
```

## Advanced Usage

### Custom Kernels for Different GPUs

```python
from jax_mxfp4.kernels import get_optimized_kernel

# Auto-select best kernel for current hardware
kernel = get_optimized_kernel(device_type=jax.devices()[0].device_kind)

# Use optimized kernel
output = kernel(input, weight_packed, weight_scales)
```

### Mixed Precision Training

```python
from jax_mxfp4.training import MXFP4Trainer

trainer = MXFP4Trainer(
    model=model,
    optimizer=optax.adam(1e-4),
    gradient_accumulation_steps=4,
)

# Train with automatic quantization
for batch in dataloader:
    loss, grads = trainer.train_step(batch)
```

### Analyze Quantization Error

```python
from jax_mxfp4 import analyze_quantization_error

# Check quantization impact
stats = analyze_quantization_error(weight, block_size=32)
print(f"MSE: {stats['mse']:.6f}")
print(f"Compression: {stats['compression_ratio']:.1f}x")
```

## Performance Benchmarks

| Model | FP32 Size | MXFP4 Size | Compression | Speedup |
|-------|-----------|------------|-------------|---------|
| GPT-2 | 1.5 GB | 190 MB | 7.9x | 1.8x |
| GPT-J | 24 GB | 3.1 GB | 7.7x | 2.1x |
| GPT-OSS-20B | 86 GB | 11 GB | 7.8x | 2.3x |

## MXFP4 Format Details

```
Block Structure (32 values):
┌─────────────────────────┐
│ Shared Exponent (8-bit) │
├─────────────────────────┤
│ 32 × 4-bit values       │
│ [Sign:1][Mantissa:3]    │
└─────────────────────────┘
```

## Supported Hardware

- ✅ **NVIDIA GPUs**: H100, A100, V100, T4
- ✅ **Google TPUs**: v3, v4, v5
- ✅ **AMD GPUs**: MI250X, MI300 (experimental)
- ✅ **CPU**: Fallback implementation

## API Reference

### Core Functions

- `quantize_to_mxfp4(tensor, block_size=32)` - Quantize to MXFP4
- `dequantize_mxfp4(packed, scales, shape)` - Dequantize from MXFP4
- `quantize_model_weights(weights_dict)` - Quantize entire model

### Layers

- `MXFP4Linear` - Quantized linear layer
- `MXFP4Embedding` - Quantized embedding layer
- `MXFP4Attention` - Quantized attention layer

### Kernels

- `mxfp4_matmul` - Quantized matrix multiplication
- `fused_mxfp4_gelu` - Fused quantized GELU

## Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## Citation

If you use JAX-MXFP4 in your research, please cite:

```bibtex
@software{jax-mxfp4,
  title = {JAX-MXFP4: 4-bit Quantization for JAX},
  author = {Atsentia Team},
  year = {2025},
  url = {https://github.com/atsentia/jax-mxfp4}
}
```

## License

MIT License - see [LICENSE](LICENSE) file for details.

## Acknowledgments

- JAX team for the amazing framework
- OpenAI for GPT-OSS-20B model
- NVIDIA for Triton and quantization research
