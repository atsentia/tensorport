{
  "config": {
    "head_dim": 64,
    "hidden_act": "silu",
    "hidden_size": 2880,
    "intermediate_size": 2880,
    "layer_types": [
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention",
      "attention"
    ],
    "max_position_embeddings": 131072,
    "num_attention_heads": 64,
    "num_experts_per_tok": 4,
    "num_hidden_layers": 24,
    "num_key_value_heads": 8,
    "num_local_experts": 32,
    "quantization_method": "mxfp4",
    "rms_norm_eps": 1e-6,
    "rope_theta": 150000.0,
    "sliding_window": 128,
    "tie_word_embeddings": true,
    "use_kv_cache": true,
    "vocab_size": 201088
  },
  "format": "numpy_direct",
  "tensors": [
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_22_mlp_up_proj_weight.npy",
      "name": "model.layers.22.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_mlp_down_proj_weight.npy",
      "name": "model.layers.1.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_post_attention_layernorm_weight.npy",
      "name": "model.layers.1.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_self_attn_o_proj_weight.npy",
      "name": "model.layers.2.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_20_self_attn_k_proj_weight.npy",
      "name": "model.layers.20.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_21_self_attn_v_proj_weight.npy",
      "name": "model.layers.21.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_12_mlp_gate_proj_weight.npy",
      "name": "model.layers.12.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_9_self_attn_v_proj_weight.npy",
      "name": "model.layers.9.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_17_self_attn_o_proj_weight.npy",
      "name": "model.layers.17.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_21_mlp_gate_proj_weight.npy",
      "name": "model.layers.21.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_18_mlp_up_proj_weight.npy",
      "name": "model.layers.18.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_10_post_attention_layernorm_weight.npy",
      "name": "model.layers.10.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_10_self_attn_q_proj_weight.npy",
      "name": "model.layers.10.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_12_mlp_down_proj_weight.npy",
      "name": "model.layers.12.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_input_layernorm_weight.npy",
      "name": "model.layers.1.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_9_self_attn_q_proj_weight.npy",
      "name": "model.layers.9.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_21_input_layernorm_weight.npy",
      "name": "model.layers.21.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_14_self_attn_v_proj_weight.npy",
      "name": "model.layers.14.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_18_self_attn_q_proj_weight.npy",
      "name": "model.layers.18.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_3_self_attn_q_proj_weight.npy",
      "name": "model.layers.3.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_15_mlp_up_proj_weight.npy",
      "name": "model.layers.15.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_4_self_attn_q_proj_weight.npy",
      "name": "model.layers.4.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_15_mlp_down_proj_weight.npy",
      "name": "model.layers.15.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_5_self_attn_q_proj_weight.npy",
      "name": "model.layers.5.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_mlp_up_proj_weight.npy",
      "name": "model.layers.1.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_12_self_attn_v_proj_weight.npy",
      "name": "model.layers.12.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_16_self_attn_k_proj_weight.npy",
      "name": "model.layers.16.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_3_self_attn_k_proj_weight.npy",
      "name": "model.layers.3.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_13_mlp_down_proj_weight.npy",
      "name": "model.layers.13.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_19_mlp_up_proj_weight.npy",
      "name": "model.layers.19.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_17_post_attention_layernorm_weight.npy",
      "name": "model.layers.17.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_18_self_attn_v_proj_weight.npy",
      "name": "model.layers.18.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_22_input_layernorm_weight.npy",
      "name": "model.layers.22.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_16_mlp_down_proj_weight.npy",
      "name": "model.layers.16.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_11_self_attn_q_proj_weight.npy",
      "name": "model.layers.11.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_20_self_attn_v_proj_weight.npy",
      "name": "model.layers.20.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_mlp_down_proj_weight.npy",
      "name": "model.layers.0.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_20_mlp_down_proj_weight.npy",
      "name": "model.layers.20.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_7_input_layernorm_weight.npy",
      "name": "model.layers.7.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_9_mlp_gate_proj_weight.npy",
      "name": "model.layers.9.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_21_post_attention_layernorm_weight.npy",
      "name": "model.layers.21.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_9_mlp_up_proj_weight.npy",
      "name": "model.layers.9.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_12_post_attention_layernorm_weight.npy",
      "name": "model.layers.12.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_14_self_attn_q_proj_weight.npy",
      "name": "model.layers.14.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_self_attn_o_proj_weight.npy",
      "name": "model.layers.1.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_15_post_attention_layernorm_weight.npy",
      "name": "model.layers.15.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_14_mlp_up_proj_weight.npy",
      "name": "model.layers.14.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_10_mlp_down_proj_weight.npy",
      "name": "model.layers.10.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_18_post_attention_layernorm_weight.npy",
      "name": "model.layers.18.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_7_self_attn_k_proj_weight.npy",
      "name": "model.layers.7.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_18_mlp_down_proj_weight.npy",
      "name": "model.layers.18.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_18_self_attn_k_proj_weight.npy",
      "name": "model.layers.18.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_19_mlp_gate_proj_weight.npy",
      "name": "model.layers.19.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_22_mlp_down_proj_weight.npy",
      "name": "model.layers.22.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_18_mlp_gate_proj_weight.npy",
      "name": "model.layers.18.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_post_attention_layernorm_weight.npy",
      "name": "model.layers.2.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_8_mlp_gate_proj_weight.npy",
      "name": "model.layers.8.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_14_mlp_down_proj_weight.npy",
      "name": "model.layers.14.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_9_self_attn_o_proj_weight.npy",
      "name": "model.layers.9.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_15_self_attn_v_proj_weight.npy",
      "name": "model.layers.15.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_mlp_gate_proj_weight.npy",
      "name": "model.layers.0.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_mlp_down_proj_weight.npy",
      "name": "model.layers.2.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_6_self_attn_k_proj_weight.npy",
      "name": "model.layers.6.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_8_self_attn_v_proj_weight.npy",
      "name": "model.layers.8.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_16_self_attn_v_proj_weight.npy",
      "name": "model.layers.16.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_input_layernorm_weight.npy",
      "name": "model.layers.2.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_post_attention_layernorm_weight.npy",
      "name": "model.layers.0.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_7_mlp_gate_proj_weight.npy",
      "name": "model.layers.7.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_11_mlp_gate_proj_weight.npy",
      "name": "model.layers.11.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_self_attn_q_proj_weight.npy",
      "name": "model.layers.2.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_13_input_layernorm_weight.npy",
      "name": "model.layers.13.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_15_mlp_gate_proj_weight.npy",
      "name": "model.layers.15.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_16_input_layernorm_weight.npy",
      "name": "model.layers.16.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_17_self_attn_q_proj_weight.npy",
      "name": "model.layers.17.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_mlp_up_proj_weight.npy",
      "name": "model.layers.0.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_21_self_attn_q_proj_weight.npy",
      "name": "model.layers.21.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_22_mlp_gate_proj_weight.npy",
      "name": "model.layers.22.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_22_self_attn_o_proj_weight.npy",
      "name": "model.layers.22.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_7_self_attn_q_proj_weight.npy",
      "name": "model.layers.7.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_22_self_attn_k_proj_weight.npy",
      "name": "model.layers.22.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_input_layernorm_weight.npy",
      "name": "model.layers.0.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_self_attn_q_proj_weight.npy",
      "name": "model.layers.0.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_23_self_attn_q_proj_weight.npy",
      "name": "model.layers.23.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_5_mlp_gate_proj_weight.npy",
      "name": "model.layers.5.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_0_self_attn_k_proj_weight.npy",
      "name": "model.layers.0.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_20_self_attn_q_proj_weight.npy",
      "name": "model.layers.20.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_9_mlp_down_proj_weight.npy",
      "name": "model.layers.9.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_17_mlp_gate_proj_weight.npy",
      "name": "model.layers.17.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_11_self_attn_k_proj_weight.npy",
      "name": "model.layers.11.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_23_self_attn_o_proj_weight.npy",
      "name": "model.layers.23.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_18_self_attn_o_proj_weight.npy",
      "name": "model.layers.18.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_14_input_layernorm_weight.npy",
      "name": "model.layers.14.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_self_attn_q_proj_weight.npy",
      "name": "model.layers.1.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_6_self_attn_v_proj_weight.npy",
      "name": "model.layers.6.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_12_self_attn_k_proj_weight.npy",
      "name": "model.layers.12.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_12_self_attn_o_proj_weight.npy",
      "name": "model.layers.12.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_19_self_attn_o_proj_weight.npy",
      "name": "model.layers.19.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_10_self_attn_k_proj_weight.npy",
      "name": "model.layers.10.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_mlp_gate_proj_weight.npy",
      "name": "model.layers.2.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_18_input_layernorm_weight.npy",
      "name": "model.layers.18.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_11_post_attention_layernorm_weight.npy",
      "name": "model.layers.11.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_8_input_layernorm_weight.npy",
      "name": "model.layers.8.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_13_mlp_gate_proj_weight.npy",
      "name": "model.layers.13.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_20_mlp_gate_proj_weight.npy",
      "name": "model.layers.20.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_7_self_attn_o_proj_weight.npy",
      "name": "model.layers.7.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_mlp_gate_proj_weight.npy",
      "name": "model.layers.1.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_6_mlp_gate_proj_weight.npy",
      "name": "model.layers.6.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_7_mlp_up_proj_weight.npy",
      "name": "model.layers.7.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_4_self_attn_k_proj_weight.npy",
      "name": "model.layers.4.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_12_mlp_up_proj_weight.npy",
      "name": "model.layers.12.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_6_mlp_up_proj_weight.npy",
      "name": "model.layers.6.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_12_self_attn_q_proj_weight.npy",
      "name": "model.layers.12.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_6_self_attn_o_proj_weight.npy",
      "name": "model.layers.6.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_7_self_attn_v_proj_weight.npy",
      "name": "model.layers.7.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_5_mlp_up_proj_weight.npy",
      "name": "model.layers.5.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_8_mlp_up_proj_weight.npy",
      "name": "model.layers.8.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_10_self_attn_v_proj_weight.npy",
      "name": "model.layers.10.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_15_input_layernorm_weight.npy",
      "name": "model.layers.15.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_self_attn_k_proj_weight.npy",
      "name": "model.layers.2.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_6_mlp_down_proj_weight.npy",
      "name": "model.layers.6.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_5_self_attn_k_proj_weight.npy",
      "name": "model.layers.5.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_13_self_attn_v_proj_weight.npy",
      "name": "model.layers.13.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_17_self_attn_v_proj_weight.npy",
      "name": "model.layers.17.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_10_input_layernorm_weight.npy",
      "name": "model.layers.10.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_19_input_layernorm_weight.npy",
      "name": "model.layers.19.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_self_attn_v_proj_weight.npy",
      "name": "model.layers.1.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_13_mlp_up_proj_weight.npy",
      "name": "model.layers.13.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_16_mlp_up_proj_weight.npy",
      "name": "model.layers.16.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_23_input_layernorm_weight.npy",
      "name": "model.layers.23.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_20_post_attention_layernorm_weight.npy",
      "name": "model.layers.20.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_8_self_attn_o_proj_weight.npy",
      "name": "model.layers.8.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_10_mlp_gate_proj_weight.npy",
      "name": "model.layers.10.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_15_self_attn_q_proj_weight.npy",
      "name": "model.layers.15.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_23_self_attn_k_proj_weight.npy",
      "name": "model.layers.23.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_17_mlp_down_proj_weight.npy",
      "name": "model.layers.17.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_11_self_attn_v_proj_weight.npy",
      "name": "model.layers.11.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_12_input_layernorm_weight.npy",
      "name": "model.layers.12.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_14_mlp_gate_proj_weight.npy",
      "name": "model.layers.14.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_11_input_layernorm_weight.npy",
      "name": "model.layers.11.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_1_self_attn_k_proj_weight.npy",
      "name": "model.layers.1.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_10_mlp_up_proj_weight.npy",
      "name": "model.layers.10.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_23_mlp_gate_proj_weight.npy",
      "name": "model.layers.23.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_3_post_attention_layernorm_weight.npy",
      "name": "model.layers.3.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_4_post_attention_layernorm_weight.npy",
      "name": "model.layers.4.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_6_input_layernorm_weight.npy",
      "name": "model.layers.6.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_11_self_attn_o_proj_weight.npy",
      "name": "model.layers.11.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_16_mlp_gate_proj_weight.npy",
      "name": "model.layers.16.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_20_self_attn_o_proj_weight.npy",
      "name": "model.layers.20.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_22_post_attention_layernorm_weight.npy",
      "name": "model.layers.22.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_8_self_attn_q_proj_weight.npy",
      "name": "model.layers.8.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_2_mlp_up_proj_weight.npy",
      "name": "model.layers.2.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_14_post_attention_layernorm_weight.npy",
      "name": "model.layers.14.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_22_self_attn_q_proj_weight.npy",
      "name": "model.layers.22.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_23_post_attention_layernorm_weight.npy",
      "name": "model.layers.23.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_4_mlp_down_proj_weight.npy",
      "name": "model.layers.4.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_21_self_attn_k_proj_weight.npy",
      "name": "model.layers.21.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_000/model_layers_11_mlp_up_proj_weight.npy",
      "name": "model.layers.11.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_embed_tokens_weight.npy",
      "name": "model.embed_tokens.weight",
      "shape": [
        201088,
        2880
      ],
      "size_mb": 1104.609375
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_14_self_attn_o_proj_weight.npy",
      "name": "model.layers.14.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_19_self_attn_k_proj_weight.npy",
      "name": "model.layers.19.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_4_mlp_up_proj_weight.npy",
      "name": "model.layers.4.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_6_post_attention_layernorm_weight.npy",
      "name": "model.layers.6.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_4_input_layernorm_weight.npy",
      "name": "model.layers.4.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_0_self_attn_v_proj_weight.npy",
      "name": "model.layers.0.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_17_input_layernorm_weight.npy",
      "name": "model.layers.17.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_17_self_attn_k_proj_weight.npy",
      "name": "model.layers.17.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_20_mlp_up_proj_weight.npy",
      "name": "model.layers.20.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_5_post_attention_layernorm_weight.npy",
      "name": "model.layers.5.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_13_self_attn_o_proj_weight.npy",
      "name": "model.layers.13.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_16_post_attention_layernorm_weight.npy",
      "name": "model.layers.16.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_19_mlp_down_proj_weight.npy",
      "name": "model.layers.19.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_3_self_attn_v_proj_weight.npy",
      "name": "model.layers.3.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_6_self_attn_q_proj_weight.npy",
      "name": "model.layers.6.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_23_mlp_down_proj_weight.npy",
      "name": "model.layers.23.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_7_post_attention_layernorm_weight.npy",
      "name": "model.layers.7.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_8_mlp_down_proj_weight.npy",
      "name": "model.layers.8.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_23_self_attn_v_proj_weight.npy",
      "name": "model.layers.23.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_5_self_attn_v_proj_weight.npy",
      "name": "model.layers.5.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_16_self_attn_o_proj_weight.npy",
      "name": "model.layers.16.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_20_input_layernorm_weight.npy",
      "name": "model.layers.20.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_21_mlp_up_proj_weight.npy",
      "name": "model.layers.21.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_8_self_attn_k_proj_weight.npy",
      "name": "model.layers.8.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_10_self_attn_o_proj_weight.npy",
      "name": "model.layers.10.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_9_post_attention_layernorm_weight.npy",
      "name": "model.layers.9.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_9_self_attn_k_proj_weight.npy",
      "name": "model.layers.9.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_5_self_attn_o_proj_weight.npy",
      "name": "model.layers.5.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_3_mlp_gate_proj_weight.npy",
      "name": "model.layers.3.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_14_self_attn_k_proj_weight.npy",
      "name": "model.layers.14.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_23_mlp_up_proj_weight.npy",
      "name": "model.layers.23.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_4_self_attn_o_proj_weight.npy",
      "name": "model.layers.4.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_19_self_attn_v_proj_weight.npy",
      "name": "model.layers.19.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_9_input_layernorm_weight.npy",
      "name": "model.layers.9.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_13_self_attn_q_proj_weight.npy",
      "name": "model.layers.13.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_15_self_attn_k_proj_weight.npy",
      "name": "model.layers.15.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_4_self_attn_v_proj_weight.npy",
      "name": "model.layers.4.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_5_mlp_down_proj_weight.npy",
      "name": "model.layers.5.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_3_mlp_down_proj_weight.npy",
      "name": "model.layers.3.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_5_input_layernorm_weight.npy",
      "name": "model.layers.5.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_4_mlp_gate_proj_weight.npy",
      "name": "model.layers.4.mlp.gate_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_11_mlp_down_proj_weight.npy",
      "name": "model.layers.11.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_7_mlp_down_proj_weight.npy",
      "name": "model.layers.7.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_8_post_attention_layernorm_weight.npy",
      "name": "model.layers.8.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_16_self_attn_q_proj_weight.npy",
      "name": "model.layers.16.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_13_post_attention_layernorm_weight.npy",
      "name": "model.layers.13.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_21_self_attn_o_proj_weight.npy",
      "name": "model.layers.21.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_22_self_attn_v_proj_weight.npy",
      "name": "model.layers.22.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_2_self_attn_v_proj_weight.npy",
      "name": "model.layers.2.self_attn.v_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_19_post_attention_layernorm_weight.npy",
      "name": "model.layers.19.post_attention_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_19_self_attn_q_proj_weight.npy",
      "name": "model.layers.19.self_attn.q_proj.weight",
      "shape": [
        4096,
        2880
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_13_self_attn_k_proj_weight.npy",
      "name": "model.layers.13.self_attn.k_proj.weight",
      "shape": [
        512,
        2880
      ],
      "size_mb": 2.8125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_0_self_attn_o_proj_weight.npy",
      "name": "model.layers.0.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_17_mlp_up_proj_weight.npy",
      "name": "model.layers.17.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_15_self_attn_o_proj_weight.npy",
      "name": "model.layers.15.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_21_mlp_down_proj_weight.npy",
      "name": "model.layers.21.mlp.down_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_3_mlp_up_proj_weight.npy",
      "name": "model.layers.3.mlp.up_proj.weight",
      "shape": [
        2880,
        2880
      ],
      "size_mb": 15.8203125
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_norm_weight.npy",
      "name": "model.norm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_3_input_layernorm_weight.npy",
      "name": "model.layers.3.input_layernorm.weight",
      "shape": [
        2880
      ],
      "size_mb": 0.0054931640625
    },
    {
      "dtype": "<f2",
      "file": "shard_001/model_layers_3_self_attn_o_proj_weight.npy",
      "name": "model.layers.3.self_attn.o_proj.weight",
      "shape": [
        2880,
        4096
      ],
      "size_mb": 22.5
    }
  ],
  "total_parameters": 1813481280,
  "total_shards": 2
}